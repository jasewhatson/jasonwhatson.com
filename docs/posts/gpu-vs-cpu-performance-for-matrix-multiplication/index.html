<!doctype html>
<html lang="en-us">
  <head>
    <title>GPU vs CPU Performance for Matrix Multiplication // My thoughts</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.105.0">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Jase Whatson" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.97cc403ebb6d07ce62dd7b32d6885dd18da10797c13e2442fa776caa98425e70.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="GPU vs CPU Performance for Matrix Multiplication"/>
<meta name="twitter:description" content="Introduction: Unleashing the Power of Parallelism in AI The field of artificial intelligence has undergone a revolution in recent years, driven in large part by advancements in hardware and the innovative architectures they enable. Traditional models like Long Short-Term Memory networks (LSTMs) were once state-of-the-art, but their sequential nature limited their ability to leverage parallel processing. Enter Transformers: a game-changing architecture that has reshaped the AI landscape by fully embracing the power of parallel computation."/>

    <meta property="og:title" content="GPU vs CPU Performance for Matrix Multiplication" />
<meta property="og:description" content="Introduction: Unleashing the Power of Parallelism in AI The field of artificial intelligence has undergone a revolution in recent years, driven in large part by advancements in hardware and the innovative architectures they enable. Traditional models like Long Short-Term Memory networks (LSTMs) were once state-of-the-art, but their sequential nature limited their ability to leverage parallel processing. Enter Transformers: a game-changing architecture that has reshaped the AI landscape by fully embracing the power of parallel computation." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jasonwhatson.com/posts/gpu-vs-cpu-performance-for-matrix-multiplication/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-12-28T13:41:59+11:00" />
<meta property="article:modified_time" content="2024-12-28T13:41:59+11:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://jasonwhatson.com"><img class="app-header-avatar" src="/avatar.jpg" alt="Jase Whatson" /></a>
      <span class="app-header-title">My thoughts</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/tags/">Tags</a>
      </nav>
      <p>Documenting my journey as a developer into AI, web3, blockchain, finance, cryptographic consensus &amp; related topics. DYOR</p>
      <div class="app-header-social">
        
          <a href="https://github.com/jasewhatson" target="_blank" rel="noreferrer noopener me">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
          <a href="https://twitter.com/jasewhatson" target="_blank" rel="noreferrer noopener me">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>Twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">GPU vs CPU Performance for Matrix Multiplication</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Dec 28, 2024
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          9 min read
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://jasonwhatson.com/tags/deep-learning/">Deep-Learning</a>
              <a class="tag" href="https://jasonwhatson.com/tags/gpu/">GPU</a>
              <a class="tag" href="https://jasonwhatson.com/tags/matrix-multiplication/">Matrix-Multiplication</a>
              <a class="tag" href="https://jasonwhatson.com/tags/parallel-computing/">Parallel-Computing</a>
              <a class="tag" href="https://jasonwhatson.com/tags/neural-networks/">Neural-Networks</a>
              <a class="tag" href="https://jasonwhatson.com/tags/artificial-intelligence/">Artificial-Intelligence</a>
              <a class="tag" href="https://jasonwhatson.com/tags/linear-algebra/">Linear-Algebra</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <h2 id="introduction-unleashing-the-power-of-parallelism-in-ai">Introduction: Unleashing the Power of Parallelism in AI</h2>
<p>The field of artificial intelligence has undergone a revolution in recent years, driven in large part by advancements in hardware and the innovative architectures they enable. Traditional models like Long Short-Term Memory networks (LSTMs) were once state-of-the-art, but their sequential nature limited their ability to leverage parallel processing. Enter Transformers: a game-changing architecture that has reshaped the AI landscape by fully embracing the power of parallel computation.</p>
<p>At the heart of this transformation is the GPU (Graphics Processing Unit). Originally designed for rendering photorealistic video games, GPUs have become indispensable for AI research and development. Their ability to execute thousands of simple operations simultaneously makes them ideal for tasks like matrix multiplication (and other linear algebra operations), an essential building block of neural networks. This distinction is exemplified in our interactive matrix multiplication demo below, where we compare parallel (GPU-powered) and non-parallel (CPU-based) computing. The results highlight why GPUs, &amp; recently, TPU&rsquo;s (Tensor Processing Units) are the cornerstone of modern AI.</p>
<!-- ## Introduction: Unleashing the Power of Parallelism in AI - Claude fix

The field of artificial intelligence has undergone a revolution in recent years, driven by significant advancements in hardware and the innovative architectures they enable. Traditional models like Long Short-Term Memory networks (LSTMs), while capable of running on GPUs, were limited by their inherently sequential nature in processing information. The emergence of Transformers marked a pivotal shift in AI architecture, not only by fully embracing parallel computation but also by introducing sophisticated self-attention mechanisms that could better handle long-range dependencies in data.

At the heart of this transformation is the GPU (Graphics Processing Unit). Originally designed for rendering photorealistic video games, GPUs have become indispensable for AI research and development. Unlike CPUs, which have fewer but more complex cores, GPUs contain thousands of smaller, simpler cores optimized for parallel processing. This specialized architecture makes them exceptionally efficient at executing thousands of simple operations simultaneously, particularly for tasks like matrix multiplication and other linear algebra operations - the essential building blocks of neural networks. This fundamental difference is demonstrated in our interactive matrix multiplication demo below, where we compare parallel (GPU-powered) and non-parallel (CPU-based) computing. The results highlight why GPUs have become the cornerstone of modern AI development and training. -->
<h2 id="why-lstms-hit-a-wall">Why LSTMs Hit a Wall</h2>
<p>Long Short-Term Memory networks (LSTMs), while revolutionary for processing sequential data, face several fundamental limitations such as Sequential Processing Bottlenecks.</p>
<p>LSTMs are inherently sequential models. Each input token depends on the state produced by the previous token, making it impossible to process sequences in parallel. For example, in a 1000-token sequence, token #500 cannot be processed until all 499 previous tokens have been computed.</p>
<p>This sequential dependency results in slower training times and inefficiencies, particularly for long sequences. While modern GPUs excel at parallel computation, LSTMs cannot fully utilize this capability due to their architecture. The recursive nature of LSTMs means that every token must pass through the same network repeatedly, compounding these limitations.</p>
<p>Alternitivly, Modern Transformer architectures &amp; GPU&rsquo;s a have largely addressed these limitations through self-attention mechanisms and parallel processing capabilities, enabling efficient processing of much longer sequences.</p>
<p>Unlike CPUs, which have fewer but more complex cores, GPUs contain thousands of smaller, simpler cores optimized for parallel processing. This specialized architecture makes them exceptionally efficient at executing thousands of simple operations simultaneously, particularly for tasks like matrix multiplication and other linear algebra operations - the essential building blocks of neural networks.</p>
<h2 id="how-transformers-overcame-sequential-processing-limitations">How Transformers Overcame Sequential Processing Limitations</h2>
<p>Transformers, introduced in the 2017 paper “Attention Is All You Need,” revolutionized sequence processing through two key innovations. Positional embeddings that encode token order, and the self-attention mechanism that calculates relationships between all tokens in parallel.</p>
<p>The parallel processing power of Transformers leverages two key computing concepts:</p>
<h3 id="simd-single-instruction-multiple-data-architecture">SIMD (Single Instruction Multiple Data) Architecture</h3>
<p>Deep learning models process vast amounts of data using mathematical operations. At their core, these operations rely heavily on matrix multiplications to transform input data into meaningful representations. This aligns perfectly with modern GPU architectures optimized for SIMD operations, enabling attention calculations to execute in parallel across multiple tokens.</p>
<h3 id="embarrassingly-parallel-nature">Embarrassingly Parallel Nature</h3>
<p>Self-attention computations can be split into completely independent calculations, with multiple attention heads processing different relationship aspects with no dependencies. These highly optimized matrix operations on specialized hardware (TPUs, GPU Tensor Cores) enable parallelization in both training and inference.</p>
<p>This parallel architecture explains why Transformers scale so effectively to massive models and datasets compared to sequential architectures like LSTMs. The shift from O(n) sequential steps to parallel processing enabled training on unprecedented amounts of data, enabling new AI applications like modern Large Language Models (LLM&rsquo;s).</p>
<h2 id="matrix-operations-the-core-of-ai-parallelism">Matrix Operations: The Core of AI Parallelism</h2>
<p>The magic of Transformers lies in their utilisation of matrix operations—particularly within the self-attention mechanism. Tasks like calculating the relationships between queries, keys, and values in attention layers involve operations such as <a href="https://www.codecademy.com/resources/docs/numpy/built-in-functions/dot">dot products</a> between large matrices. GPUs excel at matrix math because:</p>
<ul>
<li>Massive Parallelism: GPUs have thousands of simple cores designed for executing parallel operations.</li>
<li>High Memory Bandwidth: Modern GPUs can move data quickly between memory and processing units, a critical factor for large-scale computations.</li>
<li>Vectorization: Operations on entire arrays (e.g., tensors) can be performed simultaneously, avoiding the inefficiency of loops.</li>
</ul>
<h2 id="why-gpus-changed-the-game">Why GPUs Changed the Game</h2>
<p>Before GPUs were repurposed for AI, CPUs were the primary drivers of computation. While CPUs are versatile, their limited core count and reliance on sequential processing made them ill-suited for the parallelism required by neural networks. GPUs, on the other hand, are optimized for embarrassingly parallel tasks, such as rendering 3D scenes and performing massive matrix multiplications. The introduction of CUDA in 2007 allowed researchers to unlock the full potential of GPUs for AI workloads, replacing clusters of CPUs with smaller, more powerful GPU setups.</p>
<blockquote>
<p>Between 1990 and 2010, off-the-shelf CPUs became faster by a factor of approximately 5,000. As a result, nowadays it’s possible to run small deep learning models on your laptop, whereas this would have been intractable 25 years ago. But typical deep learning models used in computer vision or speech recognition require orders of magnitude more computational power than your laptop can deliver.</p>
</blockquote>
<blockquote>
<p>Throughout the 2000s, companies like NVIDIA and AMD invested billions of dollars
in developing fast, massively parallel chips (graphical processing units, or GPUs) to
power the graphics of increasingly photorealistic video games—cheap, single-purpose
supercomputers designed to render complex 3D scenes on your screen in real time.
This investment came to benefit the scientific community when, in 2007, NVIDIA
launched <a href="https://developer.nvidia.com/about-cuda">CUDA</a>, a programming interface
for its line of GPUs. A small number of GPUs started replacing massive clusters of
CPUs in various highly parallelizable applications, beginning with physics modeling.
Deep neural networks, consisting mostly of many small matrix multiplications, are
also highly parallelizable, and around 2011 some researchers began to write CUDA
implementations of neural nets—Dan Ciresan and Alex Krizhevsky were among
the first. <cite> - François Chollet <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></cite></p>
</blockquote>
<h2 id="scalability-and-modern-ai">Scalability and Modern AI</h2>
<p>Transformers are inherently scalable, and GPUs are their perfect companion. From the attention mechanism to feedforward layers, every step in the Transformer architecture benefits from parallelism. This synergy between hardware and architecture enabled the training of modern Large Language Models (LLMs) like GPT and BERT, which have billions of parameters. Without GPUs, training these models on massive datasets would take months or even years.</p>
<h2 id="the-demo-parallel-computing-in-action">The Demo: Parallel Computing in Action</h2>
<p>To truly appreciate the difference between parallel and sequential computing, check out the below matrix multiplication demo</p>
<ul>
<li>CPU (Non-Parallel): Processes matrix elements sequentially, with performance bottlenecks as input size grows.</li>
<li>GPU (Parallel): Leverages thousands of cores to compute results simultaneously, demonstrating the massive speedup achieved through parallelism. For the purposes of this demo we are using four parraell procceses</li>
</ul>
<p>This demonstration illustrates why GPUs are indispensable for deep learning, and why the Transformer arcitectue has replaced LSTMs as the foundation of modern AI.</p>

<!--<!DOCTYPE html>
 <html>

<head>
    <title>Matrix Multiplication: CPU vs GPU</title> -->
    <style>
        /* body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            margin: 20px;
            background: #f5f5f5;
        } */

        .container {
            max-width: 1200px;
            margin: 0 auto;
        }

        .controls {
            display: flex;
            justify-content: center;
            gap: 1rem;
            margin-bottom: 2rem;
        }

        .button {
            padding: 0.5rem 1rem;
            border: none;
            border-radius: 0.375rem;
            color: white;
            cursor: pointer;
            font-size: 1rem;
        }

        .button-run {
            background: #3b82f6;
        }

        .button-run:hover {
            background: #2563eb;
        }

        .button-step {
            background: #22c55e;
        }

        .button-step:hover {
            background: #16a34a;
        }

        .button-reset {
            background: #ef4444;
        }

        .button-reset:hover {
            background: #dc2626;
        }

        .computation-section {
            background: #242930;
            border-radius: 0.5rem;
            padding: 1rem;
            margin-bottom: 2rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .section-title {
            text-align: center;
            font-size: 1.125rem;
            font-weight: bold;
            margin-bottom: 1rem;
        }

        .stats {
            text-align: center;
            font-family: monospace;
            margin-bottom: 0.5rem;
        }

        .calculation {
            text-align: center;
            font-family: monospace;
            font-size: 0.875rem;
            margin-bottom: 1rem;
            min-height: 1.5em;
        }

        .matrices {
            display: flex;
            justify-content: center;
            gap: 2rem;
            flex-wrap: wrap;
        }

        .matrix-container {
            text-align: center;
        }

        .matrix-label {
            font-weight: bold;
            margin-bottom: 0.5rem;
        }

        .matrix {
            display: grid;
            grid-template-columns: repeat(3, 3rem);
            gap: 2px;
            /*background: #f3f4f6;*/    
            padding: 0.5rem;
            border-radius: 0.25rem;
        }

        .cell {
            width: 3rem;
            height: 3rem;
            display: flex;
            align-items: center;
            justify-content: center;
            /*background: white;*/
            border: 1px solid rgb(175, 186, 196);
            font-family: monospace;
            color: white;
        }

        .cell.active {
            background: #1f9d4e; /* #22c55e; /* #bbf7d0;*/
             
        }

        .cell.computed {
            background: #3b82f6; /* #bfdbfe; */
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="controls">
            <button id="runButton" class="button button-run">Run Animation</button>
            <button id="stepButton" class="button button-step">Step</button>
            <button id="resetButton" class="button button-reset">Reset</button>
        </div>

        <!-- GPU Section -->
        <div class="computation-section">
            <div class="section-title">GPU Computation (Parallel)</div>
            <div id="gpuStats" class="stats">Elements computed: 0 / 9</div>
            <div id="gpuCalc" class="calculation"></div>
            <div class="matrices">
                <div class="matrix-container">
                    <div class="matrix-label">Matrix A</div>
                    <div id="gpuMatrixA" class="matrix"></div>
                </div>
                <div class="matrix-container">
                    <div class="matrix-label">Matrix B</div>
                    <div id="gpuMatrixB" class="matrix"></div>
                </div>
                <div class="matrix-container">
                    <div class="matrix-label">Result</div>
                    <div id="gpuMatrixC" class="matrix"></div>
                </div>
            </div>
        </div>

        <!-- CPU Section -->
        <div class="computation-section">
            <div class="section-title">CPU Computation (Sequential)</div>
            <div id="cpuStats" class="stats">Elements computed: 0 / 9</div>
            <div id="cpuCalc" class="calculation"></div>
            <div class="matrices">
                <div class="matrix-container">
                    <div class="matrix-label">Matrix A</div>
                    <div id="cpuMatrixA" class="matrix"></div>
                </div>
                <div class="matrix-container">
                    <div class="matrix-label">Matrix B</div>
                    <div id="cpuMatrixB" class="matrix"></div>
                </div>
                <div class="matrix-container">
                    <div class="matrix-label">Result</div>
                    <div id="cpuMatrixC" class="matrix"></div>
                </div>
            </div>
        </div>
    </div>

    <script>
        const size = 3;
        let currentStep = 0;
        let isRunning = false;
        let animationId = null;

        const matrixA = [
            [1, 2, 3],
            [4, 5, 6],
            [7, 8, 9]
        ];

        const matrixB = [
            [9, 8, 7],
            [6, 5, 4],
            [3, 2, 1]
        ];

        const resultMatrix = [
            [30, 24, 18],
            [84, 69, 54],
            [138, 114, 90]
        ];

        // Initialize computed states
        const gpuComputed = Array(size).fill().map(() => Array(size).fill(false));
        const cpuComputed = Array(size).fill().map(() => Array(size).fill(false));

        function createMatrix(id, data) {
            const matrix = document.getElementById(id);
            matrix.innerHTML = '';
            data.forEach((row, i) => {
                row.forEach((value, j) => {
                    const cell = document.createElement('div');
                    cell.className = 'cell';
                    cell.textContent = value;
                    matrix.appendChild(cell);
                });
            });
        }

        function calculateCell(row, col) {
            let sum = 0;
            let calc = '';
            for (let k = 0; k < size; k++) {
                sum += matrixA[row][k] * matrixB[k][col];
                calc += k === 0 ? `${matrixA[row][k]}×${matrixB[k][col]}` : ` + ${matrixA[row][k]}×${matrixB[k][col]}`;
            }
            return { result: sum, calculation: `${calc} = ${sum}` };
        }

        function clearHighlights(prefix) {
            document.querySelectorAll(`#${prefix}MatrixA .cell, #${prefix}MatrixB .cell`)
                .forEach(cell => cell.className = 'cell');
        }

        function highlightComputation(row, col, prefix) {
            clearHighlights(prefix);

            // Highlight current row in matrix A
            for (let k = 0; k < size; k++) {
                const cellA = document.querySelector(`#${prefix}MatrixA .cell:nth-child(${row * size + k + 1})`);
                if (cellA) cellA.className = 'cell active';
            }

            // Highlight current column in matrix B
            for (let k = 0; k < size; k++) {
                const cellB = document.querySelector(`#${prefix}MatrixB .cell:nth-child(${k * size + col + 1})`);
                if (cellB) cellB.className = 'cell active';
            }

            // Mark result cell as computed
            const resultCell = document.querySelector(`#${prefix}MatrixC .cell:nth-child(${row * size + col + 1})`);
            if (resultCell) {
                resultCell.className = 'cell computed';
                resultCell.textContent = resultMatrix[row][col];
            }
        }

        function countComputed(matrix) {
            return matrix.reduce((sum, row) =>
                sum + row.reduce((s, cell) => s + (cell ? 1 : 0), 0), 0);
        }

        function updateStats() {
            document.getElementById('gpuStats').textContent =
                `Elements computed: ${countComputed(gpuComputed)} / ${size * size}`;
            document.getElementById('cpuStats').textContent =
                `Elements computed: ${countComputed(cpuComputed)} / ${size * size}`;
        }

        // Modified step() function
        function step() {
            if (currentStep >= size * size) {
                isRunning = false;
                return;
            }

            // GPU computation (parallel) - always process 4 elements if available
            const gpuBatchSize = 4;
            let gpuCalculations = [];

            for (let i = 0; i < gpuBatchSize && (currentStep * gpuBatchSize + i) < size * size; i++) {
                const index = currentStep * gpuBatchSize + i;
                const row = Math.floor(index / size);
                const col = index % size;

                if (row < size && col < size) {
                    gpuComputed[row][col] = true;
                    const { calculation } = calculateCell(row, col);
                    gpuCalculations.push(calculation);
                    highlightComputation(row, col, 'gpu');
                }
            }

            // CPU computation (sequential)
            const cpuRow = Math.floor(currentStep / size);
            const cpuCol = currentStep % size;
            if (cpuRow < size && cpuCol < size) {
                cpuComputed[cpuRow][cpuCol] = true;
                const { calculation } = calculateCell(cpuRow, cpuCol);
                document.getElementById('cpuCalc').textContent = calculation;
                highlightComputation(cpuRow, cpuCol, 'cpu');
            }

            document.getElementById('gpuCalc').textContent = gpuCalculations.join(' | ');
            updateStats();

            // Update step counter
            currentStep++;

            // Check if computation is complete
            if (countComputed(gpuComputed) >= size * size && countComputed(cpuComputed) >= size * size) {
                isRunning = false;
                document.getElementById('runButton').textContent = 'Run Animation';
            }
        }

        // Update the run() function's animation timing
        function run() {
            isRunning = !isRunning;
            document.getElementById('runButton').textContent = isRunning ? 'Pause' : 'Run Animation';

            if (isRunning) {
                function animate() {
                    if (!isRunning) return;
                    step();
                    //if (currentStep * 4 < size * size) {  // Updated condition
                    if (currentStep < size * size) {  // Updated condition
                        setTimeout(() => requestAnimationFrame(animate), 1000);  // Increased delay for better visualization
                    } else {
                        isRunning = false;
                        document.getElementById('runButton').textContent = 'Run Animation';
                    }
                }
                animate();
            }
        }

        function reset() {
            currentStep = 0;
            isRunning = false;
            document.getElementById('runButton').textContent = 'Run Animation';

            // Reset computed states
            for (let i = 0; i < size; i++) {
                for (let j = 0; j < size; j++) {
                    gpuComputed[i][j] = false;
                    cpuComputed[i][j] = false;
                }
            }

            // Clear highlights and reset result matrices
            clearHighlights('gpu');
            clearHighlights('cpu');

            // Reset calculations
            document.getElementById('gpuCalc').textContent = '';
            document.getElementById('cpuCalc').textContent = '';

            // Reset result matrices
            createMatrix('gpuMatrixC', Array(size).fill().map(() => Array(size).fill(0)));
            createMatrix('cpuMatrixC', Array(size).fill().map(() => Array(size).fill(0)));

            updateStats();
        }

        // Initialize matrices
        createMatrix('gpuMatrixA', matrixA);
        createMatrix('gpuMatrixB', matrixB);
        createMatrix('gpuMatrixC', Array(size).fill().map(() => Array(size).fill(0)));
        createMatrix('cpuMatrixA', matrixA);
        createMatrix('cpuMatrixB', matrixB);
        createMatrix('cpuMatrixC', Array(size).fill().map(() => Array(size).fill(0)));

        // Add event listeners
        document.getElementById('runButton').addEventListener('click', run);
        document.getElementById('stepButton').addEventListener('click', step);
        document.getElementById('resetButton').addEventListener('click', reset);

        // Initial stats update
        updateStats();
    </script>
<!-- </body>

</html> -->
<h2 id="conclusion">Conclusion</h2>
<p>The shift from LSTMs to Transformers exemplifies how embracing parallelism revolutionized AI. By leveraging GPU-optimized architectures and operations like matrix multiplication, researchers have dramatically increased the scale, speed, and efficiency of training neural networks. The Transformer arcitectures ability to process sequences in parallel not only solved the inefficiencies of sequential models but also enabled the creation of groundbreaking LLMs. This leap in computational efficiency has paved the way for the explosive growth of AI, transforming industries and inspiring innovations that were once thought impossible.</p>
<p>Artificial Neural Networks (ANNs), inspired by biological neurons, benefit immensely from GPUs because their core operations—primarily matrix multiplications—are inherently parallel. GPUs/TPUs are specifically designed for such workloads, with thousands of lightweight cores optimized for parallel matrix math and high memory bandwidth, making them ideal for the computational demands of AI.</p>
<h2 id="cuda-demo-code">Cuda Demo Code</h2>
<figure style="text-align: center;">
    <img width="100%" src="/images/GPUCudavsCPU-ezgifdotcom.gif" alt="Under construction">
    <div>GPU Computation Time: 21.504 ms <br>
CPU Computation Time: 14559.604 ms <br>
GPU Total Threads = (16 × 16) × [1024/16] × [1024/16] = 256 × 64 × 64 = 1,048,576 <br>
CPU Total Threads = 1 Single Thread
</div>
</figure>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">60
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">61
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">62
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">63
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">64
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">65
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">66
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">67
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">68
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">69
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">70
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">71
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">72
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">73
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">74
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">75
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">76
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">77
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">78
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">79
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">80
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">81
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">82
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">83
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">84
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">85
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">86
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">87
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">88
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">89
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">90
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">91
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">92
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">93
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">94
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">95
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c++" data-lang="c++"><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&#34;cuda_runtime.h&#34;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&#34;device_launch_parameters.h&#34;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&lt;stdio.h&gt;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&lt;chrono&gt;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span>__global__ <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">matrixMulKernel</span>(<span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> A, <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> B, <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> C, <span style="color:#66d9ef">int</span> M, <span style="color:#66d9ef">int</span> N, <span style="color:#66d9ef">int</span> P) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> row <span style="color:#f92672">=</span> blockIdx.y <span style="color:#f92672">*</span> blockDim.y <span style="color:#f92672">+</span> threadIdx.y;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> col <span style="color:#f92672">=</span> blockIdx.x <span style="color:#f92672">*</span> blockDim.x <span style="color:#f92672">+</span> threadIdx.x;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (row <span style="color:#f92672">&lt;</span> M <span style="color:#f92672">&amp;&amp;</span> col <span style="color:#f92672">&lt;</span> P) {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">float</span> sum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0f</span>;
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> k <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; k <span style="color:#f92672">&lt;</span> N; <span style="color:#f92672">++</span>k) {
</span></span><span style="display:flex;"><span>            sum <span style="color:#f92672">+=</span> A[row <span style="color:#f92672">*</span> N <span style="color:#f92672">+</span> k] <span style="color:#f92672">*</span> B[k <span style="color:#f92672">*</span> P <span style="color:#f92672">+</span> col];
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>        C[row <span style="color:#f92672">*</span> P <span style="color:#f92672">+</span> col] <span style="color:#f92672">=</span> sum;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">void</span> <span style="color:#a6e22e">matrixMulCPU</span>(<span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> A, <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> B, <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> C, <span style="color:#66d9ef">int</span> M, <span style="color:#66d9ef">int</span> N, <span style="color:#66d9ef">int</span> P) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> row <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; row <span style="color:#f92672">&lt;</span> M; <span style="color:#f92672">++</span>row) {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> col <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; col <span style="color:#f92672">&lt;</span> P; <span style="color:#f92672">++</span>col) {
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">float</span> sum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0f</span>;
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> k <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; k <span style="color:#f92672">&lt;</span> N; <span style="color:#f92672">++</span>k) {
</span></span><span style="display:flex;"><span>                sum <span style="color:#f92672">+=</span> A[row <span style="color:#f92672">*</span> N <span style="color:#f92672">+</span> k] <span style="color:#f92672">*</span> B[k <span style="color:#f92672">*</span> P <span style="color:#f92672">+</span> col];
</span></span><span style="display:flex;"><span>            }
</span></span><span style="display:flex;"><span>            C[row <span style="color:#f92672">*</span> P <span style="color:#f92672">+</span> col] <span style="color:#f92672">=</span> sum;
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">int</span> <span style="color:#a6e22e">main</span>() {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int</span> M <span style="color:#f92672">=</span> <span style="color:#ae81ff">1024</span>; <span style="color:#75715e">// Rows in A and C
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int</span> N <span style="color:#f92672">=</span> <span style="color:#ae81ff">1024</span>; <span style="color:#75715e">// Columns in A, Rows in B
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int</span> P <span style="color:#f92672">=</span> <span style="color:#ae81ff">1024</span>; <span style="color:#75715e">// Columns in B and C
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span>    size_t sizeA <span style="color:#f92672">=</span> M <span style="color:#f92672">*</span> N <span style="color:#f92672">*</span> <span style="color:#66d9ef">sizeof</span>(<span style="color:#66d9ef">float</span>);
</span></span><span style="display:flex;"><span>    size_t sizeB <span style="color:#f92672">=</span> N <span style="color:#f92672">*</span> P <span style="color:#f92672">*</span> <span style="color:#66d9ef">sizeof</span>(<span style="color:#66d9ef">float</span>);
</span></span><span style="display:flex;"><span>    size_t sizeC <span style="color:#f92672">=</span> M <span style="color:#f92672">*</span> P <span style="color:#f92672">*</span> <span style="color:#66d9ef">sizeof</span>(<span style="color:#66d9ef">float</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> A <span style="color:#f92672">=</span> <span style="color:#66d9ef">new</span> <span style="color:#66d9ef">float</span>[M <span style="color:#f92672">*</span> N];
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> B <span style="color:#f92672">=</span> <span style="color:#66d9ef">new</span> <span style="color:#66d9ef">float</span>[N <span style="color:#f92672">*</span> P];
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> C <span style="color:#f92672">=</span> <span style="color:#66d9ef">new</span> <span style="color:#66d9ef">float</span>[M <span style="color:#f92672">*</span> P]; <span style="color:#75715e">// For GPU results
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> C_cpu <span style="color:#f92672">=</span> <span style="color:#66d9ef">new</span> <span style="color:#66d9ef">float</span>[M <span style="color:#f92672">*</span> P]; <span style="color:#75715e">// For CPU results
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// Initialize matrices
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> M <span style="color:#f92672">*</span> N; <span style="color:#f92672">++</span>i) A[i] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0f</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> N <span style="color:#f92672">*</span> P; <span style="color:#f92672">++</span>i) B[i] <span style="color:#f92672">=</span> <span style="color:#ae81ff">2.0f</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> dev_A, <span style="color:#f92672">*</span> dev_B, <span style="color:#f92672">*</span> dev_C;
</span></span><span style="display:flex;"><span>    cudaMalloc((<span style="color:#66d9ef">void</span><span style="color:#f92672">**</span>)<span style="color:#f92672">&amp;</span>dev_A, sizeA);
</span></span><span style="display:flex;"><span>    cudaMalloc((<span style="color:#66d9ef">void</span><span style="color:#f92672">**</span>)<span style="color:#f92672">&amp;</span>dev_B, sizeB);
</span></span><span style="display:flex;"><span>    cudaMalloc((<span style="color:#66d9ef">void</span><span style="color:#f92672">**</span>)<span style="color:#f92672">&amp;</span>dev_C, sizeC);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cudaMemcpy(dev_A, A, sizeA, cudaMemcpyHostToDevice);
</span></span><span style="display:flex;"><span>    cudaMemcpy(dev_B, B, sizeB, cudaMemcpyHostToDevice);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    dim3 threadsPerBlock(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">16</span>);
</span></span><span style="display:flex;"><span>    dim3 blocksPerGrid((P <span style="color:#f92672">+</span> threadsPerBlock.x <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> threadsPerBlock.x, (M <span style="color:#f92672">+</span> threadsPerBlock.y <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> threadsPerBlock.y);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cudaEvent_t start, stop;
</span></span><span style="display:flex;"><span>    cudaEventCreate(<span style="color:#f92672">&amp;</span>start);
</span></span><span style="display:flex;"><span>    cudaEventCreate(<span style="color:#f92672">&amp;</span>stop);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// GPU computation
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    cudaEventRecord(start);
</span></span><span style="display:flex;"><span>    matrixMulKernel <span style="color:#f92672">&lt;&lt;</span> <span style="color:#f92672">&lt;</span>blocksPerGrid, threadsPerBlock <span style="color:#f92672">&gt;&gt;</span> <span style="color:#f92672">&gt;</span> (dev_A, dev_B, dev_C, M, N, P);
</span></span><span style="display:flex;"><span>    cudaEventRecord(stop);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cudaMemcpy(C, dev_C, sizeC, cudaMemcpyDeviceToHost);
</span></span><span style="display:flex;"><span>    cudaEventSynchronize(stop);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">float</span> milliseconds <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>    cudaEventElapsedTime(<span style="color:#f92672">&amp;</span>milliseconds, start, stop);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// CPU computation
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">auto</span> cpu_start <span style="color:#f92672">=</span> std<span style="color:#f92672">::</span>chrono<span style="color:#f92672">::</span>high_resolution_clock<span style="color:#f92672">::</span>now();
</span></span><span style="display:flex;"><span>    matrixMulCPU(A, B, C_cpu, M, N, P);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> cpu_stop <span style="color:#f92672">=</span> std<span style="color:#f92672">::</span>chrono<span style="color:#f92672">::</span>high_resolution_clock<span style="color:#f92672">::</span>now();
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>chrono<span style="color:#f92672">::</span>duration<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">float</span>, std<span style="color:#f92672">::</span>milli<span style="color:#f92672">&gt;</span> cpu_ms <span style="color:#f92672">=</span> cpu_stop <span style="color:#f92672">-</span> cpu_start;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// Output the results
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    printf(<span style="color:#e6db74">&#34;GPU Computation Time: %.3f ms</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, milliseconds);
</span></span><span style="display:flex;"><span>    printf(<span style="color:#e6db74">&#34;CPU Computation Time: %.3f ms</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, cpu_ms.count());
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// Cleanup
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    cudaFree(dev_A);
</span></span><span style="display:flex;"><span>    cudaFree(dev_B);
</span></span><span style="display:flex;"><span>    cudaFree(dev_C);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">delete</span>[] A;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">delete</span>[] B;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">delete</span>[] C;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">delete</span>[] C_cpu;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="supporting--additional-reading-resources">Supporting &amp; additional reading resources</h2>
<p>How do Graphics Cards Work? Exploring GPU Architecture - <a href="https://www.youtube.com/watch?v=h9Z4oGN89MU">Branch Education Video</a></p>
<p>GPU Memory bandwidth requirements for machine learning - <a href="https://www.digitalocean.com/community/tutorials/gpu-memory-bandwidth">Adil Lheureux</a></p>
<p>CUDA Cores Vs Tensor Cores: Choosing The Right GPU For Machine Learning  - <a href="https://acecloud.ai/resources/blog/cuda-cores-vs-tensor-cores/">AceCloud</a></p>
<p>Nvidia CUDA in 100 Seconds for parraell computations - <a href="https://www.youtube.com/watch?v=pPStdjuYzSI">Fireship Video</a></p>
<p>Transformers (how LLMs work) explained visually (Transformer mat mul) - <a href="https://youtu.be/wjZofJX0v4M?si=uuc7uE_2MoTJD4wI&amp;t=297">3Blue1Brown</a></p>
<p>SIMD Wiki - <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">Wikipedia</a></p>
<p>What is SIMD? <a href="https://www.youtube.com/watch?v=YuUMCVX3UVE">https://www.youtube.com/watch?v=YuUMCVX3UVE</a> - <a href="https://www.youtube.com/watch?v=YuUMCVX3UVE">Joshua Weinstein Video</a></p>
<p>Embarrassingly parallel computing - <a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel">Wikipedia</a></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Deep learning with Python 2nd Edition by François Chollet - Chapeter 1.3.1 Hardware <a href="https://www.manning.com/books/deep-learning-with-python-second-edition">Manning Books</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
